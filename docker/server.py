# docker/server.py
import os
import uvicorn
import tempfile
import torch
import logging
import time
import gc
import numpy as np
import soundfile as sf
import librosa
from scipy import signal
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from TTS.api import TTS
from contextlib import asynccontextmanager
from pathlib import Path


# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Vari√°vel global para o modelo
tts_model = None

# ========== FUN√á√ïES OTIMIZADAS (RECOMENDA√á√ïES CHATGPT) ==========

def preprocess_reference_audio(input_path: str, output_path: str) -> bool:
    """
    Pr√©-processa o √°udio de refer√™ncia para formato ideal do XTTS v2:
    - 16 kHz mono PCM
    - Normaliza√ß√£o de volume
    - Remo√ß√£o de ru√≠do b√°sica
    """
    try:
        # Carregar √°udio original
        audio, orig_sr = librosa.load(input_path, sr=None, mono=False)
        
        # Converter para mono se necess√°rio
        if audio.ndim > 1:
            audio = librosa.to_mono(audio)
        
        # Resample para 16 kHz (ideal para XTTS v2)
        if orig_sr != 16000:
            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=16000)
        
        # Normaliza√ß√£o suave
        audio = audio / np.max(np.abs(audio)) * 0.8
        
        # Remo√ß√£o b√°sica de ru√≠do (filtro passa-alta leve)
        b, a = signal.butter(3, 80, btype='high', fs=16000)
        audio = signal.filtfilt(b, a, audio)
        
        # Salvar como 16-bit PCM
        sf.write(output_path, audio, 16000, subtype='PCM_16')
        
        logger.info(f"‚úÖ √Åudio de refer√™ncia processado: {orig_sr}Hz ‚Üí 16kHz mono")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro no pr√©-processamento: {e}")
        return False

def postprocess_generated_audio(input_path: str, output_path: str) -> bool:
    """
    P√≥s-processa o √°udio gerado:
    - Converte de 24kHz para 44.1kHz
    - Mono para est√©reo
    - Normaliza√ß√£o final
    """
    try:
        # Carregar √°udio gerado (normalmente 24kHz mono do XTTS v2)
        audio, sr = sf.read(input_path)
        
        logger.info(f"üìä √Åudio original: {sr}Hz, {audio.shape}")
        
        # Resample para 44.1kHz (qualidade padr√£o)
        if sr != 44100:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=44100)
        
        # Converter para est√©reo
        if audio.ndim == 1:
            audio = np.column_stack([audio, audio])
        
        # Normaliza√ß√£o final suave
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.85
        
        # Salvar como 44.1kHz est√©reo 16-bit
        sf.write(output_path, audio, 44100, subtype='PCM_16')
        
        logger.info(f"‚úÖ √Åudio p√≥s-processado: 44.1kHz est√©reo")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro no p√≥s-processamento: {e}")
        return False

def cleanup_temp_files(file_paths: list):
    """Remove arquivos tempor√°rios de forma segura"""
    for path in file_paths:
        if path and os.path.exists(path):
            try:
                os.unlink(path)
                logger.debug(f"üóëÔ∏è Arquivo removido: {path}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha ao remover {path}: {e}")

# ========== FUN√á√ïES ORIGINAIS (MANTIDAS PARA COMPATIBILIDADE) ==========

def validate_reference_audio(audio_path: str) -> dict:
    """Validar e analisar √°udio de refer√™ncia"""
    try:
        audio_data, sample_rate = sf.read(audio_path)
        
        # Converter para mono se necess√°rio
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        duration = len(audio_data) / sample_rate
        rms = np.sqrt(np.mean(audio_data**2))
        
        # Valida√ß√µes
        if duration < 3.0:
            return {"valid": False, "error": "√Åudio de refer√™ncia muito curto (m√≠nimo 3s)"}
        
        if duration > 30.0:
            return {"valid": False, "error": "√Åudio de refer√™ncia muito longo (m√°ximo 30s)"}
        
        if rms < 0.01:
            return {"valid": False, "error": "√Åudio de refer√™ncia muito baixo (aumente o volume)"}
        
        return {
            "valid": True, 
            "duration": duration,
            "sample_rate": sample_rate,
            "rms": rms,
            "recommendations": get_audio_recommendations(duration, rms)
        }
        
    except Exception as e:
        return {"valid": False, "error": f"Erro ao analisar √°udio: {str(e)}"}

def get_audio_recommendations(duration: float, rms: float) -> list:
    """Gerar recomenda√ß√µes baseadas na an√°lise do √°udio"""
    recommendations = []
    
    if duration < 5:
        recommendations.append("Use √°udio mais longo (5-15s) para melhor qualidade")
    
    if rms < 0.05:
        recommendations.append("√Åudio muito baixo - use √°udio com voz mais forte")
    
    if rms > 0.3:
        recommendations.append("√Åudio pode estar muito alto - normalize o volume")
    
    return recommendations

def remove_silence_from_audio(audio_path: str, output_path: str) -> bool:
    """Remover sil√™ncios excessivos do √°udio gerado"""
    try:
        audio_data, sample_rate = sf.read(audio_path)
        
        # Par√¢metros para detec√ß√£o de sil√™ncio
        silence_threshold = 0.01
        min_silence_duration = 0.5  # segundos
        min_silence_samples = int(min_silence_duration * sample_rate)
        
        # Detectar segmentos n√£o-silenciosos
        audio_abs = np.abs(audio_data)
        non_silent_indices = audio_abs > silence_threshold
        
        # Encontrar in√≠cio e fim de segmentos de fala
        speech_segments = []
        in_speech = False
        start = 0
        
        for i, is_speech in enumerate(non_silent_indices):
            if is_speech and not in_speech:
                start = max(0, i - int(0.1 * sample_rate))  # Padding de 0.1s
                in_speech = True
            elif not is_speech and in_speech:
                end = min(len(audio_data), i + int(0.1 * sample_rate))  # Padding de 0.1s
                if end - start > min_silence_samples:
                    speech_segments.append((start, end))
                in_speech = False
        
        # Adicionar √∫ltimo segmento se necess√°rio
        if in_speech:
            speech_segments.append((start, len(audio_data)))
        
        # Concatenar segmentos com pausas menores
        if speech_segments:
            processed_audio = []
            for i, (start, end) in enumerate(speech_segments):
                processed_audio.append(audio_data[start:end])
                # Adicionar pausa curta entre segmentos (exceto no √∫ltimo)
                if i < len(speech_segments) - 1:
                    pause_samples = int(0.3 * sample_rate)  # 0.3s de pausa
                    processed_audio.append(np.zeros(pause_samples))
            
            final_audio = np.concatenate(processed_audio)
            sf.write(output_path, final_audio, sample_rate)
            logger.info(f"üéµ Sil√™ncios removidos: {len(audio_data)/sample_rate:.1f}s ‚Üí {len(final_audio)/sample_rate:.1f}s")
            return True
        else:
            # Se n√£o encontrar fala, manter original
            sf.write(output_path, audio_data, sample_rate)
            return False
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao remover sil√™ncios: {e}")
        return False

def optimize_text_for_speech(text: str, language: str = "pt") -> str:
    """Otimizar texto para melhor s√≠ntese de voz"""
    import re
    
    # Limpar texto inicial
    text = text.strip()
    
    # Substituir abrevia√ß√µes comuns ANTES de processar pontua√ß√£o
    if language == "pt":
        replacements = {
            r'\bdr\b': 'doutor',
            r'\bdra\b': 'doutora', 
            r'\bprof\b': 'professor',
            r'\bsra\b': 'senhora',
            r'\bsr\b': 'senhor',
            r'\bvc\b': 'voc√™',
            r'\bpq\b': 'porque',
            r'\btd\b': 'tudo',
            r'\btbm\b': 'tamb√©m',
            r'\bqdo\b': 'quando',
            r'\bmto\b': 'muito',
            r'\bpf\b': 'por favor',
            # N√∫meros por extenso (b√°sico)
            r'\b1\b': 'um',
            r'\b2\b': 'dois', 
            r'\b3\b': 'tr√™s',
            r'\b4\b': 'quatro',
            r'\b5\b': 'cinco',
            r'\b10\b': 'dez',
            r'\b100\b': 'cem'
        }
        
        for pattern, replacement in replacements.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Limpar pontua√ß√£o problem√°tica
    # Remover m√∫ltiplos pontos seguidos
    text = re.sub(r'\.{2,}', '.', text)
    # Remover espa√ßos antes de pontua√ß√£o
    text = re.sub(r'\s+([.!?,:;])', r'\1', text)
    # Garantir espa√ßo ap√≥s pontua√ß√£o
    text = re.sub(r'([.!?,:;])([A-Za-z])', r'\1 \2', text)
    
    # Remover pontua√ß√£o redundante no final
    text = re.sub(r'[.!?]+$', '', text)

def get_character_limits() -> dict:
    """Limites de caracteres por idioma para XTTS"""
    return {
        "pt": 200,  # Portugu√™s - limite conservador para melhor qualidade
        "en": 250,  # Ingl√™s
        "es": 220,  # Espanhol
        "fr": 230,  # Franc√™s
        "de": 180,  # Alem√£o
        "it": 210,  # Italiano
        "pl": 190,  # Polon√™s
        "tr": 200,  # Turco
        "ru": 180,  # Russo
        "nl": 200,  # Holand√™s
        "cs": 190,  # Tcheco
        "ar": 150,  # √Årabe
        "zh": 100,  # Chin√™s
        "ja": 120,  # Japon√™s
        "hi": 150,  # Hindi
        "ko": 130   # Coreano
    }

def smart_text_split(text: str, language: str = "pt", max_chars: int = None) -> list:
    """Dividir texto inteligentemente respeitando limites por idioma"""
    limits = get_character_limits()
    if max_chars is None:
        max_chars = limits.get(language, 200)
    
    # Se texto est√° dentro do limite, retornar como est√°
    if len(text) <= max_chars:
        return [text]
    
    # Dividir por senten√ßas primeiro
    import re
    
    # Padr√µes de divis√£o por idioma
    sentence_patterns = {
        "pt": r'[.!?]+\s+',
        "en": r'[.!?]+\s+',
        "es": r'[.!?]+\s+',
        "fr": r'[.!?]+\s+',
        "de": r'[.!?]+\s+',
        "it": r'[.!?]+\s+',
    }
    
    pattern = sentence_patterns.get(language, r'[.!?]+\s+')
    sentences = re.split(pattern, text.strip())
    
    # Reagrupar senten√ßas em chunks menores que o limite
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # Adicionar pontua√ß√£o se n√£o houver
        if not sentence.endswith(('.', '!', '?')):
            sentence += '.'
        
        # Se senten√ßa sozinha j√° excede limite, dividir por v√≠rgulas
        if len(sentence) > max_chars:
            sub_parts = sentence.split(',')
            for part in sub_parts:
                part = part.strip()
                if len(part) > max_chars:
                    # Se ainda muito grande, dividir for√ßadamente
                    for i in range(0, len(part), max_chars - 10):
                        chunk_part = part[i:i + max_chars - 10].strip()
                        if chunk_part:
                            chunks.append(chunk_part + '.')
                else:
                    if len(current_chunk + part) > max_chars:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = part + ', '
                    else:
                        current_chunk += part + ', '
        else:
            # Verificar se pode adicionar √† chunk atual
            test_chunk = current_chunk + sentence + ' '
            if len(test_chunk) > max_chars:
                # Salvar chunk atual e iniciar nova
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ' '
            else:
                current_chunk += sentence + ' '
    
    # Adicionar √∫ltima chunk se houver
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # Garantir que nenhuma chunk excede o limite
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > max_chars:
            # Divis√£o for√ßada como √∫ltimo recurso
            for i in range(0, len(chunk), max_chars - 10):
                sub_chunk = chunk[i:i + max_chars - 10].strip()
                if sub_chunk:
                    final_chunks.append(sub_chunk)
        else:
            final_chunks.append(chunk)
    
    logger.info(f"üìù Texto dividido em {len(final_chunks)} chunks (limite: {max_chars} chars)")
    for i, chunk in enumerate(final_chunks):
        logger.info(f"   Chunk {i+1}: {len(chunk)} chars - '{chunk[:50]}...'")
    
    return final_chunks

def concatenate_audio_chunks(chunk_paths: list, output_path: str, crossfade_duration: float = 0.1) -> bool:
    """Concatenar chunks de √°udio com crossfade suave e valida√ß√£o rigorosa"""
    try:
        if not chunk_paths:
            return False
        
        if len(chunk_paths) == 1:
            # Se apenas um chunk, copiar diretamente
            audio_data, sr = sf.read(chunk_paths[0])
            sf.write(output_path, audio_data, sr)
            return True
        
        logger.info(f"üîó Concatenando {len(chunk_paths)} chunks com crossfade de {crossfade_duration}s")
        
        # Validar todos os chunks primeiro
        validated_chunks = []
        target_sr = None
        
        for i, chunk_path in enumerate(chunk_paths):
            try:
                audio_data, sr = sf.read(chunk_path)
                
                # Validar sample rate
                if target_sr is None:
                    target_sr = sr
                elif sr != target_sr:
                    logger.warning(f"‚ö†Ô∏è Chunk {i+1} tem sample rate diferente: {sr} vs {target_sr}")
                    # Resample para o sample rate alvo
                    audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)
                
                # Validar se √°udio n√£o est√° vazio ou muito curto
                if len(audio_data) < target_sr * 0.1:  # Menos que 100ms
                    logger.warning(f"‚ö†Ô∏è Chunk {i+1} muito curto ({len(audio_data)/target_sr:.3f}s) - ignorando")
                    continue
                
                # Normalizar volume do chunk
                max_val = np.max(np.abs(audio_data))
                if max_val > 0:
                    audio_data = audio_data / max_val * 0.8
                
                validated_chunks.append((audio_data, target_sr))
                logger.debug(f"‚úÖ Chunk {i+1} validado: {len(audio_data)/target_sr:.2f}s")
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar chunk {i+1}: {e}")
                continue
        
        if len(validated_chunks) == 0:
            logger.error("‚ùå Nenhum chunk v√°lido encontrado")
            return False
        
        if len(validated_chunks) == 1:
            # Se apenas um chunk v√°lido, usar ele
            sf.write(output_path, validated_chunks[0][0], validated_chunks[0][1])
            return True
        
        # Concatenar chunks com crossfade inteligente
        final_audio = validated_chunks[0][0]
        final_sr = validated_chunks[0][1]
        
        crossfade_samples = int(crossfade_duration * final_sr)
        
        for i in range(1, len(validated_chunks)):
            next_audio, next_sr = validated_chunks[i]
            
            # Detectar final de fala no √°udio atual e in√≠cio no pr√≥ximo
            current_end_silence = detect_silence_at_end(final_audio, final_sr)
            next_start_silence = detect_silence_at_start(next_audio, next_sr)
            
            # Ajustar crossfade baseado no sil√™ncio detectado
            effective_crossfade = min(crossfade_samples, 
                                    len(final_audio) - current_end_silence,
                                    next_start_silence)
            
            if effective_crossfade > 0 and len(final_audio) > effective_crossfade and len(next_audio) > effective_crossfade:
                # Aplicar crossfade
                fade_out_region = final_audio[-effective_crossfade:].copy()
                fade_in_region = next_audio[:effective_crossfade].copy()
                
                # Curvas de fade suaves
                fade_out_curve = np.linspace(1.0, 0.0, effective_crossfade) ** 1.5
                fade_in_curve = np.linspace(0.0, 1.0, effective_crossfade) ** 1.5
                
                # Aplicar fades
                fade_out_region *= fade_out_curve
                fade_in_region *= fade_in_curve
                
                # Misturar regi√µes
                mixed_region = fade_out_region + fade_in_region
                
                # Concatenar: √°udio atual (sem final) + regi√£o mista + pr√≥ximo √°udio (sem in√≠cio)
                final_audio = np.concatenate([
                    final_audio[:-effective_crossfade],
                    mixed_region,
                    next_audio[effective_crossfade:]
                ])
                
                logger.debug(f"üîó Chunk {i+1} concatenado com crossfade de {effective_crossfade/final_sr:.3f}s")
                
            else:
                # Se crossfade n√£o √© poss√≠vel, adicionar pausa pequena
                pause_duration = 0.05  # 50ms
                pause_samples = int(pause_duration * final_sr)
                pause = np.zeros(pause_samples)
                
                final_audio = np.concatenate([final_audio, pause, next_audio])
                logger.debug(f"üîó Chunk {i+1} concatenado com pausa de {pause_duration}s")
        
        # Normaliza√ß√£o final suave
        max_val = np.max(np.abs(final_audio))
        if max_val > 0:
            final_audio = final_audio / max_val * 0.85
        
        # Salvar resultado
        sf.write(output_path, final_audio, final_sr)
        
        total_duration = len(final_audio) / final_sr
        logger.info(f"‚úÖ {len(validated_chunks)} chunks concatenados - Dura√ß√£o total: {total_duration:.2f}s")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao concatenar chunks: {e}")
        return False

def validate_generated_chunk(chunk_path: str, chunk_number: int) -> bool:
    """Validar se um chunk de √°udio foi gerado corretamente"""
    try:
        # Verificar se arquivo existe e n√£o est√° vazio
        if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) == 0:
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_number}: arquivo vazio ou inexistente")
            return False
        
        # Carregar e analisar √°udio
        audio_data, sr = sf.read(chunk_path)
        
        if len(audio_data) == 0:
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_number}: √°udio vazio ap√≥s leitura")
            return False
        
        duration = len(audio_data) / sr
        
        # Verificar dura√ß√£o m√≠nima
        if duration < 0.1:  # Menos que 100ms
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_number}: muito curto ({duration:.3f}s)")
            return False
        
        # Verificar se h√° conte√∫do de √°udio (n√£o √© s√≥ sil√™ncio)
        rms = np.sqrt(np.mean(audio_data**2))
        
        if rms < 0.001:  # Muito baixo, provavelmente sil√™ncio
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_number}: muito baixo/silencioso (RMS: {rms:.6f})")
            return False
        
        # Verificar se n√£o h√° clipping excessivo
        clipping_ratio = np.sum(np.abs(audio_data) > 0.95) / len(audio_data)
        if clipping_ratio > 0.1:  # Mais de 10% clipping
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_number}: clipping excessivo ({clipping_ratio:.2%})")
            return False
        
        # Verificar se n√£o h√° mudan√ßas bruscas excessivas (indicativo de artifacts)
        diff = np.diff(audio_data)
        max_diff = np.max(np.abs(diff))
        if max_diff > 0.5:  # Mudan√ßa muito brusca
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_number}: poss√≠veis artifacts (max_diff: {max_diff:.3f})")
            return False
        
        logger.debug(f"‚úÖ Chunk {chunk_number} validado: {duration:.2f}s, RMS: {rms:.4f}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao validar chunk {chunk_number}: {e}")
        return False

def detect_silence_at_end(audio: np.ndarray, sr: int, silence_threshold: float = 0.01) -> int:
    """Detectar samples de sil√™ncio no final do √°udio"""
    if len(audio) == 0:
        return 0
    
    # Procurar do final para o in√≠cio
    for i in range(len(audio) - 1, -1, -1):
        if abs(audio[i]) > silence_threshold:
            return len(audio) - i - 1
    
    return len(audio)  # Todo o √°udio √© sil√™ncio

def detect_silence_at_start(audio: np.ndarray, sr: int, silence_threshold: float = 0.01) -> int:
    """Detectar samples de sil√™ncio no in√≠cio do √°udio"""
    if len(audio) == 0:
        return 0
    
    # Procurar do in√≠cio para o final
    for i in range(len(audio)):
        if abs(audio[i]) > silence_threshold:
            return i
    
    return len(audio)  # Todo o √°udio √© sil√™ncio

# ========== FUN√á√ïES ORIGINAIS DE CORRE√á√ÉO (MANTIDAS) ==========

def fix_audio_hoarseness(audio_path: str, output_path: str) -> bool:
    """Corrigir rouquid√£o no √°udio"""
    try:
        audio_data, sr = sf.read(audio_path)
        
        # Normalizar volume para evitar satura√ß√£o
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            # Normalizar para 85% do m√°ximo para evitar clipping
            audio_data = audio_data * (0.85 / max_val)
        
        # Aplicar filtro passa-alta suave para reduzir ru√≠do baixo
        from scipy import signal
        
        # Filtro passa-alta em 80Hz para remover ru√≠do de fundo
        nyquist = sr / 2
        low_cutoff = 80 / nyquist
        
        if low_cutoff < 0.99:  # Verificar se frequ√™ncia √© v√°lida
            b, a = signal.butter(2, low_cutoff, btype='high')
            audio_data = signal.filtfilt(b, a, audio_data)
        
        # Suavizar picos que causam rouquid√£o
        # Detec√ß√£o de picos excessivos
        threshold = np.std(audio_data) * 3
        peaks = np.abs(audio_data) > threshold
        
        if np.any(peaks):
            # Suavizar apenas os picos detectados
            window_size = int(0.005 * sr)  # 5ms de janela
            for i in np.where(peaks)[0]:
                start = max(0, i - window_size // 2)
                end = min(len(audio_data), i + window_size // 2)
                if end > start:
                    # Aplicar suaviza√ß√£o gaussiana na regi√£o do pico
                    audio_data[start:end] = signal.savgol_filter(
                        audio_data[start:end], 
                        min(11, end - start) if (end - start) % 2 == 1 else min(10, end - start - 1), 
                        3
                    )
        
        # Compress√£o suave para uniformizar din√¢mica
        # Compressor simples: reduzir apenas volumes muito altos
        compression_threshold = 0.7
        compression_ratio = 0.6
        
        mask = np.abs(audio_data) > compression_threshold
        audio_data[mask] = np.sign(audio_data[mask]) * (
            compression_threshold + 
            (np.abs(audio_data[mask]) - compression_threshold) * compression_ratio
        )
        
        sf.write(output_path, audio_data, sr)
        logger.info("üéõÔ∏è Corre√ß√£o de rouquid√£o aplicada")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao corrigir rouquid√£o: {e}")
        return False

# ========== APP SETUP ==========

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global tts_model
    logger.info("üöÄ Iniciando servidor Coqui TTS...")
    
    # Verifica√ß√£o detalhada de GPU
    logger.info(f"üîç PyTorch version: {torch.__version__}")
    logger.info(f"üîç CUDA compiled version: {torch.version.cuda}")
    logger.info(f"üîç CUDA available: {torch.cuda.is_available()}")
    logger.info(f"üîç CUDA device count: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        device = "cuda"
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"üì± Dispositivo: {device}")
        logger.info(f"üéÆ GPU: {gpu_name}")
        logger.info(f"üíæ VRAM Total: {gpu_memory:.1f}GB")
        
        # Teste simples de GPU
        try:
            test_tensor = torch.randn(10, 10).cuda()
            logger.info(f"‚úÖ Teste de GPU bem-sucedido")
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            logger.error(f"‚ùå Erro no teste de GPU: {e}")
            device = "cpu"
    else:
        device = "cpu"
        logger.warning("‚ö†Ô∏è GPU n√£o dispon√≠vel - usando CPU")
        logger.info("üì± Dispositivo: cpu")
        
        # Dicas para resolver problema de GPU
        logger.info("üí° Para usar GPU:")
        logger.info("   - Certifique-se que nvidia-docker est√° instalado")
        logger.info("   - Use: docker run --gpus all ...")
        logger.info("   - Ou: nvidia-docker run ...")
    
    try:
        logger.info("üì• Carregando modelo XTTS-v2...")
        start_time = time.time()
        
        # For√ßar uso de GPU se dispon√≠vel
        use_gpu = torch.cuda.is_available()
        logger.info(f"üéØ Carregando modelo com GPU: {use_gpu}")
        
        # Carregar modelo com tratamento de erro melhorado
        tts_model = TTS(
            "tts_models/multilingual/multi-dataset/xtts_v2", 
            gpu=use_gpu
        )
        
        # Verificar se modelo foi carregado na GPU
        if use_gpu and hasattr(tts_model, 'synthesizer') and hasattr(tts_model.synthesizer, 'tts_model'):
            model_device = next(tts_model.synthesizer.tts_model.parameters()).device
            logger.info(f"üéØ Modelo carregado no dispositivo: {model_device}")
        
        load_time = time.time() - start_time
        logger.info(f"‚úÖ Modelo carregado em {load_time:.2f}s!")
        
        # Verificar se o modelo tem os m√©todos necess√°rios
        if not hasattr(tts_model, 'tts_to_file'):
            raise AttributeError("Modelo n√£o possui m√©todo tts_to_file")
            
    except Exception as e:
        logger.error(f"‚ùå Erro ao carregar modelo: {e}")
        logger.error(f"Tipo de erro: {type(e).__name__}")
        raise
    
    yield
    
    # Shutdown
    logger.info("üî• Desligando servidor...")
    if tts_model:
        del tts_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

# Criar app FastAPI
app = FastAPI(
    title="Coqui TTS Server",
    description="Servidor de s√≠ntese de voz com XTTS-v2",
    version="2.0.0",
    lifespan=lifespan
)

# CORS para desenvolvimento
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {
        "message": "Coqui TTS Server", 
        "status": "running",
        "version": "2.0.0",
        "model": "XTTS-v2"
    }

@app.get("/health")
async def health_check():
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_used": f"{torch.cuda.memory_allocated(0) / 1024**3:.2f}GB",
            "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB",
            "gpu_memory_free": f"{(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3:.2f}GB"
        }
    
    return {
        "status": "healthy",
        "model_loaded": tts_model is not None,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "gpu_info": gpu_info,
        "temp_dir": str(tempfile.gettempdir())
    }

# ========== ENDPOINT OTIMIZADO (RECOMENDA√á√ïES CHATGPT) ==========

@app.post("/generate_v2")
async def generate_audio_v2(
    text: str = Form(..., description="Texto para sintetizar"),
    reference_audio: UploadFile = File(..., description="√Åudio de refer√™ncia"),
    language: str = Form("pt", description="C√≥digo do idioma"),
    # Par√¢metros otimizados para qualidade/estabilidade
    temperature: float = Form(0.75, description="Temperatura (0.65-0.85)"),
    length_penalty: float = Form(1.0, description="Penalidade de comprimento"),
    repetition_penalty: float = Form(8.0, description="Penalidade de repeti√ß√£o"),
    top_k: int = Form(20, description="Top-k sampling"),
    top_p: float = Form(0.70, description="Top-p sampling"),
    speed: float = Form(1.0, description="Velocidade da fala"),
    remove_silence: bool = Form(True, description="Remover sil√™ncios excessivos"),
):
    if not tts_model:
        raise HTTPException(status_code=503, detail="Modelo n√£o carregado")
    
    # Valida√ß√µes b√°sicas
    if not text.strip():
        raise HTTPException(status_code=400, detail="Texto vazio")
    
    if len(text) > 1000:
        raise HTTPException(status_code=400, detail="Texto muito longo")
    
    # Validar formato do arquivo
    allowed_extensions = ('.wav', '.mp3', '.flac', '.ogg', '.m4a')
    if not reference_audio.filename.lower().endswith(allowed_extensions):
        raise HTTPException(
            status_code=400, 
            detail=f"Formato n√£o suportado. Use: {', '.join(allowed_extensions)}"
        )
    
    # Validar par√¢metros otimizados
    if not (0.65 <= temperature <= 0.85):
        raise HTTPException(status_code=400, detail="Temperatura deve estar entre 0.65-0.85")
    
    if not (0.6 <= top_p <= 0.8):
        raise HTTPException(status_code=400, detail="Top-p deve estar entre 0.6-0.8")
    
    if not (10 <= top_k <= 40):
        raise HTTPException(status_code=400, detail="Top-k deve estar entre 10-40")
    
    start_time = time.time()
    ref_path = None
    processed_ref_path = None
    output_path = None
    final_path = None
    
    try:
        logger.info(f"üéµ TTS v2 Otimizado - Texto: {len(text)} chars, Idioma: {language}")
        
        # 1. Salvar √°udio de refer√™ncia original
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as ref_file:
            content = await reference_audio.read()
            ref_file.write(content)
            ref_path = ref_file.name
        
        # 2. Pr√©-processar √°udio de refer√™ncia para 16kHz mono
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as processed_ref_file:
            processed_ref_path = processed_ref_file.name
        
        if not preprocess_reference_audio(ref_path, processed_ref_path):
            raise HTTPException(status_code=400, detail="Falha no pr√©-processamento do √°udio de refer√™ncia")
        
        # 3. Otimizar texto para s√≠ntese
        optimized_text = optimize_text_for_speech(text, language)
        
        # 4. Dividir texto em chunks se necess√°rio
        char_limits = get_character_limits()
        char_limit = char_limits.get(language, 200)
        
        if len(optimized_text) > char_limit:
            text_chunks = smart_text_split(optimized_text, language, char_limit)
        else:
            text_chunks = [optimized_text]
        
        logger.info(f"üìù Texto dividido em {len(text_chunks)} chunks")
        
        # 5. Gerar √°udio para cada chunk
        chunk_paths = []
        
        for i, chunk_text in enumerate(text_chunks):
            logger.info(f"üé§ Processando chunk {i+1}/{len(text_chunks)}")
            
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as chunk_file:
                chunk_path = chunk_file.name
            
            # S√≠ntese com configura√ß√µes otimizadas
            tts_model.tts_to_file(
                text=chunk_text.strip(),
                speaker_wav=processed_ref_path,  # Usa √°udio pr√©-processado
                language=language,
                file_path=chunk_path,
                temperature=temperature,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                speed=speed,
                split_sentences=True if len(chunk_text) > 200 else False  # Split autom√°tico para textos longos
            )
            
            # Verificar se arquivo foi gerado corretamente
            if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) == 0:
                logger.error(f"‚ùå Falha na gera√ß√£o do chunk {i+1}")
                continue
            
            chunk_paths.append(chunk_path)
            logger.info(f"‚úÖ Chunk {i+1} gerado")
        
        if not chunk_paths:
            raise HTTPException(status_code=500, detail="Falha na gera√ß√£o de todos os chunks")
        
        # 6. Concatenar chunks se houver m√∫ltiplos
        if len(chunk_paths) > 1:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as concat_file:
                concatenated_path = concat_file.name
            
            if concatenate_audio_chunks(chunk_paths, concatenated_path):
                # Limpar chunks individuais
                for chunk_path in chunk_paths:
                    try:
                        os.unlink(chunk_path)
                    except:
                        pass
                output_path = concatenated_path
                logger.info(f"üîó {len(chunk_paths)} chunks concatenados")
            else:
                # Se concatena√ß√£o falhou, usar primeiro chunk
                output_path = chunk_paths[0]
                # Limpar outros chunks
                for chunk_path in chunk_paths[1:]:
                    try:
                        os.unlink(chunk_path)
                    except:
                        pass
        else:
            output_path = chunk_paths[0]
        
        # 7. P√≥s-processar para 44.1kHz est√©reo
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as final_file:
            final_path = final_file.name
        
        if not postprocess_generated_audio(output_path, final_path):
            # Fallback: usar √°udio original se p√≥s-processamento falhar
            final_path = output_path
            logger.warning("‚ö†Ô∏è Usando √°udio sem p√≥s-processamento")
        
        # 8. Remover sil√™ncios se solicitado
        if remove_silence:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as silence_file:
                silence_path = silence_file.name
            
            if remove_silence_from_audio(final_path, silence_path):
                if final_path != output_path:  # S√≥ remove se n√£o for o original
                    os.unlink(final_path)
                final_path = silence_path
        
        # Verificar arquivo final
        if not os.path.exists(final_path) or os.path.getsize(final_path) == 0:
            raise HTTPException(status_code=500, detail="Falha na gera√ß√£o do √°udio")
        
        # Estat√≠sticas finais
        generation_time = time.time() - start_time
        file_size = os.path.getsize(final_path)
        
        try:
            final_audio, final_sr = sf.read(final_path)
            duration = len(final_audio) / final_sr
            logger.info(f"‚úÖ √Åudio v2 gerado em {generation_time:.2f}s - {duration:.1f}s @ {final_sr}Hz")
        except:
            logger.info(f"‚úÖ √Åudio v2 gerado em {generation_time:.2f}s - {file_size} bytes")
        
        return FileResponse(
            final_path,
            media_type="audio/wav",
            filename=f"tts_optimized_{int(time.time())}.wav",
            headers={
                "X-Generation-Time": str(round(generation_time, 2)),
                "X-File-Size": str(file_size),
                "X-Audio-Duration": str(round(duration, 2)) if 'duration' in locals() else "unknown",
                "X-Sample-Rate": str(final_sr) if 'final_sr' in locals() else "44100",
                "X-Text-Chunks": str(len(text_chunks)),
                "X-Chunks-Generated": str(len(chunk_paths)),
                "X-Character-Limit": str(char_limit),
                "X-Original-Text-Length": str(len(text)),
                "X-Optimized-Text-Length": str(len(optimized_text)),
                "X-Text-Changed": "true" if optimized_text != text else "false",
                "X-Optimized": "true",
                "X-Version": "v2"
            },
            background=lambda: cleanup_temp_files([final_path])
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na s√≠ntese v2: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro na s√≠ntese: {str(e)}")
    
    finally:
        # Limpar arquivos tempor√°rios (exceto o final que ser√° enviado)
        cleanup_temp_files([ref_path, processed_ref_path, output_path])

# ========== ENDPOINT ORIGINAL (MANTIDO PARA COMPATIBILIDADE) ==========

@app.post("/generate")
async def generate_audio(
    text: str = Form(..., description="Texto para sintetizar"),
    reference_audio: UploadFile = File(..., description="√Åudio de refer√™ncia (.wav)"),
    language: str = Form("pt", description="C√≥digo do idioma"),
    temperature: float = Form(0.76, description="Temperatura (0.70-0.85) - mais baixa elimina rouquid√£o"),
    length_penalty: float = Form(1.0, description="Penalidade de comprimento"),
    repetition_penalty: float = Form(8.0, description="Penalidade de repeti√ß√£o (6.0-10.0)"),
    top_k: int = Form(30, description="Top-k sampling (menor = mais est√°vel)"),
    top_p: float = Form(0.75, description="Top-p sampling (menor = mais est√°vel)"),
    speed: float = Form(0.96, description="Velocidade da fala (0.9-1.1)"),
    enable_text_splitting: bool = Form(True, description="Dividir texto automaticamente"),
    remove_silence: bool = Form(True, description="Remover sil√™ncios excessivos"),
    fix_hoarseness: bool = Form(True, description="Corrigir rouquid√£o no √°udio"),
    max_chars_per_chunk: int = Form(0, description="M√°ximo de caracteres por chunk (0=autom√°tico)")
):
    """Endpoint original mantido para compatibilidade - use /generate_v2 para vers√£o otimizada"""
    if not tts_model:
        raise HTTPException(status_code=503, detail="Modelo n√£o carregado")
    
    # Valida√ß√µes b√°sicas
    if not reference_audio.filename:
        raise HTTPException(status_code=400, detail="Nome do arquivo de √°udio n√£o fornecido")
    
    allowed_extensions = ('.wav', '.mp3', '.flac', '.ogg')
    if not reference_audio.filename.lower().endswith(allowed_extensions):
        raise HTTPException(
            status_code=400, 
            detail=f"Formato de √°udio n√£o suportado. Use: {', '.join(allowed_extensions)}"
        )
    
    if len(text.strip()) == 0:
        raise HTTPException(status_code=400, detail="Texto n√£o pode estar vazio")
    
    if len(text) > 1000:
        raise HTTPException(status_code=400, detail="Texto muito longo (m√°ximo 1000 caracteres)")
    
    # Validar par√¢metros
    if not (0.70 <= temperature <= 0.85):
        raise HTTPException(status_code=400, detail="Temperatura deve estar entre 0.70 e 0.85")
    
    if not (6.0 <= repetition_penalty <= 10.0):
        raise HTTPException(status_code=400, detail="Repetition penalty deve estar entre 6.0 e 10.0")
    
    if not (0.9 <= speed <= 1.1):
        raise HTTPException(status_code=400, detail="Velocidade deve estar entre 0.9 e 1.1")
    
    if not (10 <= top_k <= 50):
        raise HTTPException(status_code=400, detail="Top-k deve estar entre 10 e 50")
    
    if not (0.65 <= top_p <= 0.85):
        raise HTTPException(status_code=400, detail="Top-p deve estar entre 0.65 e 0.85")
    
    ref_path = None
    
    try:
        logger.info(f"üéµ TTS Original - Texto: {len(text)} chars, Idioma: {language}, Temp: {temperature}")
        start_time = time.time()
        
        # Verificar limite de caracteres
        char_limits = get_character_limits()
        char_limit = char_limits.get(language, 200)
        if max_chars_per_chunk > 0:
            char_limit = min(char_limit, max_chars_per_chunk)
        
        # Otimizar texto para melhor s√≠ntese
        optimized_text = optimize_text_for_speech(text, language)
        
        # Dividir texto em chunks se necess√°rio
        if enable_text_splitting and len(optimized_text) > char_limit:
            text_chunks = smart_text_split(optimized_text, language, char_limit)
        else:
            text_chunks = [optimized_text]
        
        # Salvar √°udio de refer√™ncia temporariamente
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as ref_file:
            content = await reference_audio.read()
            ref_file.write(content)
            ref_path = ref_file.name
        
        # Validar √°udio de refer√™ncia
        validation = validate_reference_audio(ref_path)
        if not validation["valid"]:
            raise HTTPException(status_code=400, detail=f"√Åudio de refer√™ncia inv√°lido: {validation['error']}")
        
        logger.info(f"‚úÖ √Åudio de refer√™ncia v√°lido - Dura√ß√£o: {validation['duration']:.1f}s")
        
        # Processar cada chunk
        chunk_paths = []
        
        for i, chunk_text in enumerate(text_chunks):
            logger.info(f"üé§ Processando chunk {i+1}/{len(text_chunks)} ({len(chunk_text)} chars)")
            
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as chunk_file:
                chunk_path = chunk_file.name
            
            # S√≠ntese de voz
            tts_model.tts_to_file(
                text=chunk_text,
                speaker_wav=ref_path,
                language=language,
                file_path=chunk_path,
                temperature=temperature,
                length_penalty=length_penalty,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                speed=speed,
                split_sentences=False
            )
            
            # Verificar se arquivo foi gerado corretamente
            if not os.path.exists(chunk_path) or os.path.getsize(chunk_path) == 0:
                logger.error(f"‚ùå Falha na gera√ß√£o do chunk {i+1}")
                continue
            
            chunk_paths.append(chunk_path)
            logger.info(f"‚úÖ Chunk {i+1} gerado com sucesso")
        
        if not chunk_paths:
            raise HTTPException(status_code=500, detail="Falha na gera√ß√£o de todos os chunks")
        
        # Concatenar chunks se houver m√∫ltiplos
        if len(chunk_paths) > 1:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as concat_file:
                concatenated_path = concat_file.name
            
            if concatenate_audio_chunks(chunk_paths, concatenated_path):
                # Limpar chunks individuais
                for chunk_path in chunk_paths:
                    try:
                        os.unlink(chunk_path)
                    except:
                        pass
                current_path = concatenated_path
                logger.info(f"üîó {len(chunk_paths)} chunks concatenados")
            else:
                # Se concatena√ß√£o falhou, usar primeiro chunk
                current_path = chunk_paths[0]
                # Limpar outros chunks
                for chunk_path in chunk_paths[1:]:
                    try:
                        os.unlink(chunk_path)
                    except:
                        pass
        else:
            current_path = chunk_paths[0]
        
        # P√≥s-processamento
        processing_steps = []
        
        # Corrigir rouquid√£o se solicitado
        if fix_hoarseness:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fixed_file:
                fixed_path = fixed_file.name
            
            if fix_audio_hoarseness(current_path, fixed_path):
                os.unlink(current_path)
                current_path = fixed_path
                processing_steps.append("rouquid√£o corrigida")
            else:
                os.unlink(fixed_path)
        
        # Remover sil√™ncios se solicitado
        if remove_silence:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as processed_file:
                processed_path = processed_file.name
            
            if remove_silence_from_audio(current_path, processed_path):
                os.unlink(current_path)
                current_path = processed_path
                processing_steps.append("sil√™ncios removidos")
            else:
                os.unlink(processed_path)
        
        # Verificar arquivo final
        if not os.path.exists(current_path) or os.path.getsize(current_path) == 0:
            raise HTTPException(status_code=500, detail="Falha na gera√ß√£o do √°udio final")
        
        generation_time = time.time() - start_time
        file_size = os.path.getsize(current_path)
        
        # Informa√ß√µes do √°udio gerado
        try:
            final_audio, final_sr = sf.read(current_path)
            final_duration = len(final_audio) / final_sr
            logger.info(f"‚úÖ √Åudio gerado em {generation_time:.2f}s - {len(text_chunks)} chunks - Dura√ß√£o: {final_duration:.1f}s")
            if processing_steps:
                logger.info(f"üéõÔ∏è P√≥s-processamento: {', '.join(processing_steps)}")
        except:
            logger.info(f"‚úÖ √Åudio gerado em {generation_time:.2f}s - {len(text_chunks)} chunks - Tamanho: {file_size} bytes")
        
        return FileResponse(
            current_path,
            media_type="audio/wav",
            filename=f"generated_{int(time.time())}.wav",
            headers={
                "X-Generation-Time": str(generation_time),
                "X-File-Size": str(file_size),
                "X-Audio-Duration": str(final_duration) if 'final_duration' in locals() else "unknown",
                "X-Text-Chunks": str(len(text_chunks)),
                "X-Processing-Steps": ",".join(processing_steps) if processing_steps else "none",
                "X-Character-Limit": str(char_limit),
                "X-Hoarseness-Fixed": "true" if fix_hoarseness else "false",
                "X-Version": "original"
            },
            background=lambda: os.unlink(current_path) if os.path.exists(current_path) else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar √°udio: {str(e)}")
        logger.error(f"Tipo de erro: {type(e).__name__}")
        raise HTTPException(status_code=500, detail=f"Erro na s√≠ntese: {str(e)}")
    
    finally:
        # Limpar arquivo de refer√™ncia
        if ref_path and os.path.exists(ref_path):
            try:
                os.unlink(ref_path)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha ao remover arquivo de refer√™ncia {ref_path}: {e}")

# ========== OUTROS ENDPOINTS ==========

@app.get("/audio-tips")
async def get_audio_tips():
    """Dicas para melhorar a qualidade do √°udio"""
    return {
        "latest_improvements": {
            "v2_fixes": [
                "‚úÖ Pontua√ß√£o n√£o √© mais falada literalmente",
                "‚úÖ Concatena√ß√£o inteligente com detec√ß√£o de sil√™ncio",
                "‚úÖ Valida√ß√£o rigorosa de chunks individuais",
                "‚úÖ Crossfade adaptativo baseado no conte√∫do",
                "‚úÖ Limites de caracteres mais conservadores para melhor qualidade",
                "‚úÖ Divis√£o de texto inteligente preservando contexto",
                "‚úÖ Tentativas autom√°ticas de regenera√ß√£o para chunks com falhas"
            ],
            "concatenation_improvements": [
                "Detec√ß√£o autom√°tica de sil√™ncio nas bordas dos chunks",
                "Crossfade inteligente que se adapta ao conte√∫do",
                "Valida√ß√£o de sample rate e normaliza√ß√£o autom√°tica",
                "Fallback para pausas curtas quando crossfade n√£o √© poss√≠vel"
            ]
        },
        "optimization_info": {
            "v2_improvements": [
                "Pr√©-processamento do √°udio de refer√™ncia para 16kHz mono (ideal para XTTS v2)",
                "P√≥s-processamento para 44.1kHz est√©reo (qualidade universal)",
                "Sample rate correto elimina problema de voz abafada",
                "Par√¢metros otimizados para estabilidade e clareza",
                "Workflow simplificado sem corre√ß√µes desnecess√°rias"
            ],
            "recommended_endpoint": "/generate (otimizado) vs /generate_old (original)"
        },
        "text_optimization": {
            "punctuation_handling": [
                "Pontua√ß√£o redundante √© removida automaticamente",
                "Apenas um ponto final √© adicionado se necess√°rio",
                "Abrevia√ß√µes s√£o expandidas (Dr ‚Üí Doutor)",
                "N√∫meros b√°sicos s√£o convertidos para extenso"
            ],
            "chunk_splitting": [
                "Divis√£o inteligente por senten√ßa (prioridade m√°xima)",
                "Fallback para conectivos e v√≠rgulas",
                "Preserva√ß√£o do contexto entre chunks",
                "Limites mais conservadores para melhor qualidade"
            ]
        },
        "reference_audio_tips": [
            "Use √°udio de 5-15 segundos de dura√ß√£o",
            "Voz clara e bem articulada",
            "Sem ru√≠do de fundo ou eco",
            "Volume adequado (n√£o muito baixo/alto)",
            "Emo√ß√£o/energia desejada na voz",
            "Qualquer formato ser√° convertido para 16kHz mono automaticamente"
        ],
        "troubleshooting": {
            "silent_chunks": [
                "Chunks muito curtos s√£o automaticamente rejeitados",
                "Valida√ß√£o de RMS garante que h√° conte√∫do aud√≠vel",
                "Regenera√ß√£o autom√°tica com par√¢metros mais conservadores"
            ],
            "concatenation_issues": [
                "Crossfade inteligente baseado em detec√ß√£o de sil√™ncio",
                "Normaliza√ß√£o autom√°tica de volume entre chunks",
                "Fallback para pausas controladas se crossfade falhar"
            ],
            "spoken_punctuation": [
                "Pontua√ß√£o redundante √© limpa automaticamente",
                "Texto √© normalizado antes da s√≠ntese",
                "Abrevia√ß√µes s√£o expandidas para forma falada"
            ]
        },
        "parameter_tips_v2": {
            "temperature": "0.65-0.85 (padr√£o 0.75) - controlado para estabilidade",
            "repetition_penalty": "8.0 (padr√£o) - evita repeti√ß√µes",
            "speed": "1.0 (padr√£o) - velocidade natural",
            "top_k": "20 (otimizado) - mais conservador que original",
            "top_p": "0.70 (otimizado) - melhor estabilidade"
        },
        "debugging": {
            "response_headers": [
                "X-Text-Chunks: n√∫mero total de chunks criados",
                "X-Chunks-Generated: chunks que foram gerados com sucesso",
                "X-Text-Changed: se o texto foi otimizado",
                "X-Character-Limit: limite usado para divis√£o",
                "X-Original-Text-Length: tamanho do texto original",
                "X-Optimized-Text-Length: tamanho ap√≥s otimiza√ß√£o"
            ],
            "quality_indicators": [
                "Se X-Chunks-Generated < X-Text-Chunks: alguns chunks falharam",
                "Se X-Text-Changed = true: texto foi modificado para melhorar s√≠ntese",
                "Generation-Time alto pode indicar problemas de processamento"
            ]
        },
        "character_limits": get_character_limits(),
        "endpoints": {
            "/generate": "Vers√£o otimizada com todas as melhorias",
            "/generate_old": "Vers√£o original (mantida para compatibilidade)"
        }
    }

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
    